{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97c259f2",
   "metadata": {},
   "source": [
    "# Machine Learning by Example: from Start to End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6539a1d",
   "metadata": {},
   "source": [
    "# Task 1: README!\n",
    "\n",
    "#### The tasks in this notebook should be tackled in discussion with your peer group! \n",
    "\n",
    "**By asking and answering each other questions, you learn much more than doing independent work.** The article [“Embracing Digitalization: Student Learning and New Technologies” (Crittenden, Biel & Lovely 2018)](https://journals.sagepub.com/doi/10.1177/0273475318820895) shows how we learn and retain more information when we explain and discuss it with others.\n",
    "\n",
    "Before you tackle a machine learning project you should make sure you keep the bigger picture in your head - this is your human input, skill, and imagination -  no AI can do this for you at the moment. The following sketches the typical steps involved in a machine learning project:\n",
    "\n",
    "- Step 1: Frame Your Problem\n",
    "    - What is the task? - Who will use it in what environment? What are the risks and impact?\n",
    "    - How will you measure performance of your model? Measures sufficient to assess potential risks and impacts?\n",
    "    - What are the assumptions? Document and review assumptions for bias. Question everything.\n",
    "- Step 2: Get Your Data\n",
    "    - Download your data - How will your get your data from where? Permissions and licenses? Suitable and reliable?\n",
    "    - Take a quick look at the data structure - how big is it? what fields/attributes are there and how many?\n",
    "    - Set aside test data - random split or stratified split? \n",
    "- Step 3: Prepare Your Data\n",
    "    - Handling Text/Categorical Data\n",
    "    - Scaling and Transformation\n",
    "    - Separate the labels from the rest of the attributes\n",
    "- Step 4: Select and Train Your Model\n",
    "    - train and evaluate on the training set\n",
    "    - cross-validation\n",
    "- Step 6: Test on Completely New Data\n",
    "- Step 7: Publish Your Results! Party! &#x1F389; &#x1F389; &#x1F389;\n",
    "\n",
    "In this notebook, we will go through some of the key the steps. Some tasks will involve critical reflection, and others will be about coding. \n",
    "\n",
    "Remember that, **if you are taking more than 30 minutes to do one task without any progress**, you should probably take a break. \n",
    "- Note down what you did and what errors you got in a markdown cell. This will help you understand the recurring errors, you will understand where you left off when you come back to it later, and also help you when you discuss the problem with your peers and with the lab tutors.  \n",
    "\n",
    "The code in this notebook is modified from that which was made available by Aurélien Géron and his fabulous book [\"Hands-On Machine Learning with Scitkit-Learn, Keras & Tensorflow\"](https://eleanor.lib.gla.ac.uk/record=b4094676)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6025a3c8",
   "metadata": {},
   "source": [
    "## Task 1-1: Checking Your Set Up\n",
    "\n",
    "It is important not only to check that you have the correct set of software and packages, but also that the version is the right one. If versions are not compatible with your code then it will throw up errors or unexpected results.\n",
    "\n",
    "### Python\n",
    "\n",
    "Check that your Python has version greater than 3.7 using the following code. This is what the code in the noteboook requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2a627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # importing the package sys which lets you talk to your computer system.\n",
    "\n",
    "assert sys.version_info >= (3, 7) #versions are expressed a pair of numbers (3, 7) which is equivalent to 3.7. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb61a85",
   "metadata": {},
   "source": [
    "The `assert` statement throws up an error when the statement following it is not true. If it is true, nothing will be shown. Experiment by replacing the numbers in the round brackets to be much bigger. **A Pair of numbers** like `(3, 7)` in round brackets is a data structure known as a **tuple** in programming lingo. \n",
    "\n",
    "### Scikit-Learn\n",
    "\n",
    "For this part you will need to have your environment installed with Python libraries `scikit-learn` and `packaging`. Review your Codespace exercise to rememebr how to install Python libraries. Note in the code below that when importing scikit-learn in the python code, you use `sklearn`.\n",
    "\n",
    "Check that your Scikit-Learn package version is greater than 1.0.1. In this case you will need to import `version` which is part of the `packaging` Python library. This allows you to extract/parse version numbers for Python packages/libraries like `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5882a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version #import the package \"version\"\n",
    "import sklearn # import scikit-learn\n",
    "\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dafd09c",
   "metadata": {},
   "source": [
    "## Task 1-2: Review Machine Learning\n",
    "\n",
    "### Step 1: Create a markdown cell to demonstrate your own reflection\n",
    "\n",
    "- In your markdown cell embed an image or link to a diagram illustrating the workflow from data to algorithm to model and data to model to predicted output. \n",
    "    - To embed images in your markdown cell, you can use the syntax `![alt text](image.jpg)` where you replace alt text with a description of your image (keep the square brackets!) and replace image.jpg with the file path and name of your image. \n",
    "    - To include a URL, use the syntax `[title](https://www.example.com)` where you replace title  with your own description, and https://www.example.com  with your own URL. Keep all brackets intact. \n",
    "    - For your reference, you can refer to the [markdown cheat sheet](https://www.markdownguide.org/cheat-sheet/)  - note that HTML codes are also understood by your notebook.\n",
    "- Explain in your markdown cell how the examples in Lectures 3 & 4 align with the workflow. For example, what is the data, what was the learning algorithm, what is the model and what did the model output in response to new data?\n",
    "- In  your markdown cell, reflect on the range of ways to explain the workflow and the examples to a wider audience, for example, a museum curator?\n",
    "\n",
    "I've created a cell for you to use already below - double click on the area to start editing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fdf563",
   "metadata": {},
   "source": [
    "***Markdown cell to modify***\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522ddc0-38c2-4eea-8dc5-227f9e6eee4a",
   "metadata": {},
   "source": [
    "### Step 2: Discuss and report your reflection with your group\n",
    "- Get together with your peer group. Take turns to discuss your reflection above. If you have any difficulties, discuss these also.\n",
    "- Note down the results of your discussion in your notebook. In particular, note down anything that help you or others learn the topic. What approach could take in your notebook to engage the wider audience with your machine learning code.\n",
    "  \n",
    "I've created a cell for you to use already below - double click on the area to start editing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b8573-0e7e-478a-a9b4-726386dc8b37",
   "metadata": {},
   "source": [
    "***Markdown cell to modify***\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027e97d4",
   "metadata": {},
   "source": [
    "## Task 1-3: Framing the Problem\n",
    "\n",
    "In this notebook, we will be working with two datasets:\n",
    "\n",
    "1)\tTabular data consisting of information about houses in districts within the US state of California, and, \n",
    "2)\tImage pixel data, each image representing a digit handwritten by high school students and employees of the US Census Bureau. \n",
    "\n",
    "The first of these datasets will be used to **predict median housing prices for a given district**. The results of the prediction will be combined with other data to determine whether it is worth investing in a given district. \n",
    "\n",
    "The second of these datasets will be used to **classify hand written digits**. It was originally developed as a way of sorting out the handwritten US zip codes (similar to UK postcodes) at the post office. \n",
    "\n",
    "\n",
    "### Step 1: Understand how framing the problem affects data selection\n",
    "\n",
    "- The academic article [“Rethinking the field of automatic prediction of court decisions”](https://link.springer.com/article/10.1007/s10506-021-09306-3) by Medvedeva, Wieling & Vols (2023), to appreciate how, depending on the objectives, the characteristics of data and algorithm might differ. \n",
    "- Read the BBC article [“AI facial recognition: Campaigners and MPs call for ban”](https://www.bbc.co.uk/news/technology-67022005) to understand that the same data, depending on its use, can raise concern about AI. We will be discussing prediction court decisions further in Week 7.\n",
    "- In view of the above, write down your reflection on the importance of framing your problem precisely in a notebook markdown cell - not only to define the task properly, but to understand how your machine learning model will be used down the road. \n",
    "\n",
    "### Step 2: How to select your algorithm\n",
    "\n",
    "In Lecture 2, we discussed how machine learning can be divided into three types: Supervised, Unsupervised, and Reinforcement. Large part of this course is focused on supervised learning – in particular, in this notebook, we will explore this using examples of regression and classification.\n",
    "\n",
    "**To refresh your memory, read this short article from Codecademy** – [“Regression vs Classification”](https://www.codecademy.com/article/regression-vs-classification).\n",
    "\n",
    "- Discuss with your peer group whether regression or classification would fit better for predicting median housing prices.\n",
    "- Discuss with your peer group whether regression or classification would fit better for handwritten digit recognition.\n",
    "- Write down the results of the discussion. In particular, report on what you concluded after the discussion and why.\n",
    "\n",
    "### Step 3: Before Data Collection\n",
    "\n",
    "Once your problem is defined (e.g. predicting the median housing price of a district), you will need to collect a new data set appropriate for your task, and/or identify existing data sets that can be used for training your model. \n",
    "\n",
    "- Discuss with your peer group what kind of information about housing in a district you think would help predict the median housing price in the district.\n",
    "- Discuss how these decisions might depend on geographical and/or cultural differences and how the information you collect would already bias the data. \n",
    "\n",
    "**Note these down the results of the discussions in Steps 2 & 3 in a markdown cell below.** I have already created a markdown cell for your use - just double click the area to begin editing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3715aa",
   "metadata": {},
   "source": [
    "***Markdown cell to modify for Steps 1, 2 and 3***\n",
    "\n",
    "1.\n",
    "\n",
    "2.\n",
    "\n",
    "3.\n",
    "\n",
    "4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ae4ec",
   "metadata": {},
   "source": [
    "# Working with Data\n",
    "\n",
    "Overviews of machine learning and AI often make it seem as though the largest part of machine learning is in training the algorithm. This is misleading. In fact, [Forbes reported that about 80% of data science is related to data preparation](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/). This includes, among other things, data cleaning, re-scaling, and labelling.\n",
    "\n",
    "If you also include things like keeping track of what you did and why, storing and backing up data generated as well as the data used at the beginning, and recording the evaluation results, data exploration, interpretation, I would say that **95%** of machine learning is involved in **data management**. This can, in fact, be said to be a substantial part of achieving transparency, a corner stone of addressing the ethical concerns regarding AI and bias, fairness, data protection, explainability etc.\n",
    "\n",
    "So, let's get some data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15b4f64",
   "metadata": {},
   "source": [
    "# Task 2: Getting Data\n",
    "\n",
    "Before anything else, you must get the data! There many ways you can get data. The scikit-learn's `datasets` package has some datasets already available to you. You can also get data from a nuumber of places such as OECD, OpenML, Kaggle, individual repositories in GitHub. \n",
    "\n",
    "In reality, data is everywhere. In fact, artists who make use of AI often make their own datasets: for example, check out [Anna Ridler's shell images](https://annaridler.com/the-shell-record-2021), [Caroline Sinders' feminist dataset](https://carolinesinders.com/feminist-data-set/), and [Refik Anadol's coral images](https://refikanadol.com/works-old/artificial-realities-coral/). While these may be owned by the artists, it can inspire new ways of thinking about data.\n",
    "\n",
    "In this task we will look at something a little less artistic! &#x1F609;\n",
    "\n",
    "- **Example Tabular Data**: dataset comprising housing prices in California in the the United States. This dataset is available on the GitHub, courtesy of Aurelien Geron. \n",
    "- **Example Image Data**: MNIST dataset comprising images of handwritten digits. Handwritten digit recognition with the MNIST dataset is sometimes called the **\"Hello World!\" of machine learning**. \n",
    "\n",
    "We will use these datasets to carry out the prediction of housing prices and classification of digit images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656eba8c",
   "metadata": {},
   "source": [
    "## Task 2-1: Download the Data: Example Tabular Data\n",
    "\n",
    "The following code defines **function** called `load_housing_data()`. This function retrieves a compressed file avaialable at `https://github.com/ageron/data/raw/main/housing.tgz` and saves it in a folder `datasets` which is in the same folder as this notebook. This will be created if it does not exist. The code will also extract the contents in the folder `datasets`. It will then return the content of the data file `housing.csv` in the folder as a Pandas dataframe.\n",
    "\n",
    "Pandas is a powerful and popular open-source data analysis and manipulation library for the Python programming language. It provides data structures and functions needed to work with structured data, such as tabulated data, seamlessly. Here are some key features:\r\n",
    "\r\n",
    "- DataFrame: A 2-dimensional labeled data structure with columns of potentially different types, similar to a table in a database or an Excel spreadsheet.\r\n",
    "- Series: A 1-dimensional labeled array capable of holding any data type.\r\n",
    "- Data Cleaning: Tools for handling missing data, filtering, and transforming data.\r\n",
    "- Data Wrangling: Functions for merging, joining, and reshaping datasets.\r\n",
    "- Time Series: Capabilities for working with time-series data, including date range generation and frequency conversion.\r\n",
    "\r\n",
    "Pandas is widely used in data science, machine learning, and data analysis projects due to its ease of use and flexibility. ing it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ec696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "def load_housing_data(): #defines a function that loads the housing data available as .tgz file on a github URL\n",
    "    tarball_path = Path(\"datasets/housing.tgz\") # where you will save your compressed data\n",
    "    if not tarball_path.is_file():\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True) #create datasets folder if it does not exist\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\" # url of where you are getting your data from\n",
    "        urllib.request.urlretrieve(url, tarball_path) # gets the url content and saves it at location specified by tarball_path\n",
    "        with tarfile.open(tarball_path) as housing_tarball: # opens saved compressed file as housing_tarball\n",
    "            housing_tarball.extractall(path=\"datasets\") # extracts the compressed content to datasets folder\n",
    "    return pd.read_csv(Path(\"datasets/housing/housing.csv\")) #uses panadas to read the csv file from the extracted content\n",
    "\n",
    "housing = load_housing_data() #runsthe function defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e678b056",
   "metadata": {},
   "source": [
    "### If you've already downloaded and extracted the compressed file - then the following is all you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741051dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "housing = pd.read_csv(Path(\"datasets/housing/housing.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc852dd0",
   "metadata": {},
   "source": [
    "## Take a Quick Look: housing data\n",
    "\n",
    "With Pandas, you can get a summary of the data by using the method `info()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb46ae03",
   "metadata": {},
   "source": [
    "The result above tells you how many attributes (e.g. longitude, latitude) characterise the dataset. How many are there? The data type float64 is a numerical data type. So, the table above also tells you how many attributes are not numerical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c0ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts() # tells you what values the column for `ocean_proximity` can take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b5110",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.hist(bins=50, figsize=(12, 8))\n",
    "save_fig(\"attribute_histogram_plots\")  # extra code\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5900bf86",
   "metadata": {},
   "source": [
    "Finally you can run `housing.describe()` to get a summay of the data set `housing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24805732",
   "metadata": {},
   "source": [
    "At this point you should stop looking at the data until you have set aside test data. This is to prevent inadvertent bias creeping into the machine learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae81bc17",
   "metadata": {},
   "source": [
    "## Task 2-2: Download the Data: Example Image Data\n",
    "\n",
    "In contrast to tabular data, image data sets are not always read in using pandas. Technically you can do this (as the line below commented out suggests) but as there are no features human-friendly features (such as, median income etc.) - only pixel information, it does not always help to load it as a pandas dataframe, unless the model requires it to be so. Use the command `type` to see what data type `mnist` is - you can see that it is not a `pandas.core.frame.DataFrame`. Dataframes are not the preferred **data structure** for image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a9ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "\n",
    "mnist = fetch_openml('mnist_784', as_frame=False, parser='auto')\n",
    "\n",
    "#mnist_dataframe = pd.DataFrame(data=mnist.data, columns=mnist.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2311961e",
   "metadata": {},
   "source": [
    "## Take a Quick Look: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3e0e2e",
   "metadata": {},
   "source": [
    "The command `mnist.info()` will not work here, to get information about the dataset content, because it is not a pandas dataframe. However, for datasets in the `sklearn.datasets` universe, you can use the keyword `DECSR` - as demonstrated in the first code cell below. The `print` command can be used in conjunction to get some useful context of the dataset structue and origin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c410e32",
   "metadata": {},
   "source": [
    "## Task 2-3: Review the data description above with your group.\n",
    "\n",
    "What is the size of each image?\n",
    "\n",
    "Examine how LeCunn, Cortes, and Burges reorganised the NIST data as MNIST. Note that they remixed the data in two ways to create different a training dataset and test dataset. What they did do? Why do you think they did this? Was it justified? \n",
    "\n",
    "Write down the results in a markdown cell below. I have already created a markdown cell below - just double click to edit the content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55896634",
   "metadata": {},
   "source": [
    "***Mark Down cell for critique***\n",
    "\n",
    "1. \n",
    "2.\n",
    "3.\n",
    "4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5fba0",
   "metadata": {},
   "source": [
    "### To see a full list of keys other than `DESCR` which is available to this dataset You can use the command `mnist.keys()` to see more keys available - run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a1d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed70b9bc",
   "metadata": {},
   "source": [
    "## Task 2-4: Identifying the Dimension of Images\n",
    "\n",
    "You may recognise some of the keys listed above for `mnist.keys()`. The keys `data` and `target` will return the image pixel data, and the labels (that is, the categories or classification) assigned to each of these images, respectively. \n",
    "\n",
    "- Create a code cell below to use these keys in Python, to use the `shape` command to verify the number of images in the dataset (70000) and how many features (e.g. pixels) represents the image (784). Print out the shape and the target categories. \n",
    "\n",
    "I have created a cell for you below with the data and target assigned to the **variables** `images` and `categories`. Add a line to print out the shape of the images and the list a assigned categories. Single click on the area to start editing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae51ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell for python code \n",
    "\n",
    "images = mnist.data\n",
    "categories = mnist.target\n",
    "\n",
    "# insert lines below to print the shape of images and to print the categories.\n",
    "\n",
    "print(images.shape)\n",
    "print(categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f61e6",
   "metadata": {},
   "source": [
    "Let's take a look at one of the digits in the dataset - the first item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra code to visualise the image of digits\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## the code below defines a function plot_digit. The initial key work `def` stands for define, followed by function name.\n",
    "## the function take one argument image_data in a parenthesis. This is followed by a colon. \n",
    "## Each line below that will be executed when the function is used. \n",
    "## This cell only defines the function. The next cell uses the function.\n",
    "\n",
    "def plot_digit(image_data): # defines a function so that you need not type all the lines below everytime you view an image\n",
    "    image = image_data.reshape(28, 28) #reshapes the data into a 28 x 28 image - before it was a string of 784 numbers\n",
    "    plt.imshow(image, cmap=\"binary\") # show the image in black and white - binary.\n",
    "    plt.axis(\"off\") # ensures no x and y axes are displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f02abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise a selected digit with the following code\n",
    "\n",
    "some_digit = mnist.data[0]\n",
    "plot_digit(some_digit)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e690ce",
   "metadata": {},
   "source": [
    "# Task 3: Setting Aside the Test Data\n",
    "\n",
    "To set aside test data, you need to take shuffled and stratified samples. \n",
    "\n",
    "## Why Do We Shuffle\n",
    "\n",
    "The dataset you are working with could be ordered in a specific way (for example, all the data points in a specific class all at the top). If you select a percentage of 20% from the top, you could get data points in only specific classes. By shuffling we can avoid this. As it happens `sklearn` has a nifty function to allow you to split the data inclusive of splitting. This function is called `train_test_split`. See it in action below, using the housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tratio = 0.2 #to get 20% for testing and 80% for training\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=tratio, random_state=42) \n",
    "## assigning a number to random_state means that everytime you run this you get the same split, unless you change the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6040547c",
   "metadata": {},
   "source": [
    "## Why Do We Stratify\n",
    "\n",
    "If the dataset is skewed so that it contains more samples of a specific kind more than others, sampling randomly will result in your test data not representing the population you would like to test. An example of the estimated probability of getting a bad sample that does not reflect the actual population is provided below. The US population ratio of females in the census is 51.1%. The following is the probability of getting a sample with less than 48.5% or greater than 53.5% females if you take a random sample withoput stratifying: approximately **10.71%** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e27820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – shows another way to estimate the probability of bad sample\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sample_size = 1000\n",
    "ratio_female = 0.511\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "samples = (np.random.rand(100_000, sample_size) < ratio_female).sum(axis=1)\n",
    "((samples < 485) | (samples > 535)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b4372c",
   "metadata": {},
   "source": [
    "## Task 3.1: Stratified Sample: Housing Data\n",
    "\n",
    "The following code adds a column to the `housing` data to create bins of data according to interval brackets of median income of districts. This is a first step to creating a stratified sample across different income brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b0cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed7fbfe",
   "metadata": {},
   "source": [
    "The following code uses the above bins to implement startified sampling - that is, it will randomly sample 20% (because we set test ratio `tratio` to 0.2) from each income bracket defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69cc591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tratio = 0.2 #to get 20% for testing and 80% for training\n",
    "\n",
    "strat_train_set, strat_test_set = train_test_split(housing, test_size=tratio, stratify=housing[\"income_cat\"], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d7fcdc",
   "metadata": {},
   "source": [
    "The code below prints out the proportion of each income category in the stratified test set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57103983",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set) #Prints out in order of the highest proportion first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b1d994",
   "metadata": {},
   "source": [
    "Note the attribute `random_state`. Setting this to a specific number like 42 **keeps the split the same everytime you run the code**. Keep in mind that it will not stay the same if you change the underlying dataset (e.g. adding more). \n",
    "\n",
    "Discuss with your peer group, why a stratified sample based on median income is reasonable. Create a markdown cell below to report on the results of the discussion. I have already create one below, so just double click to edit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6718cca3",
   "metadata": {},
   "source": [
    "***Markdown cell***\n",
    "\n",
    "1. \n",
    "2. \n",
    "3.\n",
    "4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mnist.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270dbdb6",
   "metadata": {},
   "source": [
    "## Task 3.2: Setting Aside Test Set for Image Data \n",
    "\n",
    "In the case of `mnist` the data is already cleaned prepared, scaled and ordered, so that the training data is the first 60,000 images, followed by the test data which is the last 10,000 images. So you need not shuffle and stratify nor use `train_test_split`. Instead, you can use the following code to set aside your test dataset. The data type of `mnist.data` is `numpy.ndarray` (you can verify this with the command `type`). \n",
    "- By using a colon and then 60000 in a square bracket after `mnist.data`, you are telling the computer that you want all the items up until the 60000th item (not including the 60000th) in the array `mnist.data`. We assign this to the **variable** `X_train`. \n",
    "- Likewise, the first 60000 categories corresponding the the first 60000 images are assigned to the **variable** `y_train`. \n",
    "- By using a colon after 60000, you are telling the computer you would like all the items from the 60000th onwards.\n",
    "\n",
    "It is machine learning convention to use upper case `X` for variable names associated with data and lower case `y` in the variable names associated to labels (or categories/classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab94357",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist.data[:60000]\n",
    "y_train = mnist.target[:60000]\n",
    "\n",
    "X_test = mnist.data[60000:]\n",
    "y_test = mnist.target[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc35af8",
   "metadata": {},
   "source": [
    "# Task 4: Selecting and Training a Model\n",
    "\n",
    "You are finally ready to select and train your model. In the following code, we will use linear **regression for the prediction of district housing prices**, and a **convolutional neural network** for classification of hand written digits. For linear regression, we will use `Scikit-Learn`. For the convolutional neural networks we will use the `tensorflow` library with `keras`. Regardless of the model, the general flow is similar:  \n",
    "\n",
    "- Import the model from the relevant library. \n",
    "- Create an untrained model instance. \n",
    "- Fit the model to your training data.\n",
    "\n",
    "## Task 4-1: Housing Data and Linear Regression\n",
    "\n",
    "With linear regression, you need data, whose values are continuous - not discrete values such as categories. Note that it is not enough for the values to be numbers, which can also be categories (for example, your place in a queue is a number but is never a fraction like 1.33). The feature `income_cat` is another category expressed as a number. \n",
    "\n",
    "Before doing anything else, let's assign a copy of the stratified training set we created earlier to the variable `housing`. You should always work with copies of data and never look at the test set in case we inadvertently use information in the test to improve the performance (**data snooping bias**).\n",
    "\n",
    "To do this use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cbdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f59211",
   "metadata": {},
   "source": [
    "### Step 1: Checking Correlations: Training Set\n",
    "\n",
    "Linear regression in essence works by picking up on correlations between features. So it can be useful to explore the correlations especially between the target value you are trying to predict `median_house_value` and the other features in the dataset, e.g. `median_income`.\n",
    "\n",
    "The training set we have is of type `pandas.DataFrame`.  For pandas, dataframes have the function `corr` which calculates the correlations for you. The code is below - first it calculates all the correlations between all the pairs of features and saves it in variable `corr_matrix`. \n",
    "\n",
    "We can take a look at correlations for `median_house_value` by using that feature name in a square bracket (**with quotation marks!**). the last part `sort_values(ascending=False)` sorts the correlation to display it in descending order of correlation (that is, most correlated fetures are listed first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr(numeric_only=True) # argument is so that it only calculates for numeric value features\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e741e0d8",
   "metadata": {},
   "source": [
    "### Step 2: Visualise the Correlations\n",
    "\n",
    "Pandas also can visualise these correlations as a graph for you. In the code below, we have selected four features (see the variable with that name), to 4 x 4 grid of graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7facae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "features = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "scatter_matrix(housing[features], figsize=(12, 8))\n",
    "#save_fig(\"scatter_matrix_plot\")  \n",
    "\n",
    "#The line above is extra code you can uncomment (remove the hash at the begining) to save the image.\n",
    "#But, to use this, make sure you ran the code at the beginning of this notebook defining the save_fig function\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c4e919",
   "metadata": {},
   "source": [
    "### Step 3: Separate the Target Labels from Your Data\n",
    "\n",
    "In any machine learning task, you need to provide the data and the target Label separately to the machine learning algorithm. Otherwise, they have no way of knowing which of the features is the target label. In our scenario, the label for the housing data is the `median_house_value`. When your data is in a padas dataframe format, you can simply 1) drop the column with the label to get the data, and, 2) get the column for the target label, to get the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610bec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) ## 1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy() ## 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a9e4b6",
   "metadata": {},
   "source": [
    "### Step 4: Look for Missing Values in the Data\n",
    "\n",
    "When working with tabular data, it is quite common to find that some rows are missing values for some of the columns. If you run the `info` command for dataframes (we've done this in [Task 2-1](#Task-2-1:-Download-the-Data:-Example-Tabular-Data) above!). \n",
    "\n",
    "- Running the code will show the total number of entries. By comparing that number to the number of Non-Null entries for each feature (e.g. `total_bedrooms`) you can see whether there are missing values. \n",
    "- If there are no missing values, these numbers should be the same!\n",
    "\n",
    "How many values are missing for the number of `total_bedrooms'? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bffc540",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31181096",
   "metadata": {},
   "source": [
    "### Step 5: Handling Missing Values\n",
    "\n",
    "To handle the missing values, you need a code in place to tell the machine what to do if there are missing values. In-depth discussion of handling missing values is beyond the scope of this course, but there are three common ways of handing these:\n",
    "\n",
    "- (Option 1) Drop the row with missing value. This causes you to lose data points. In our scenario with the housing data, 168 rows will be removed.\n",
    "- (Option 2) Drop the column with missing values. This causes you to lose one of your features.\n",
    "- (Option 3) Fill in the missing value with some value such as the median or mean or fixed value that makes sense. This is called **imputing**.\n",
    "\n",
    "Depending on which approach you take, the performance of your AI could be different. Also, note that, **with Option 1, you will have to remove the corresponding rows in `housing_labels` before using these in a machine learning task**. Following are codes from each of these approaches. Read the comments included in the code for understanding what each cell is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ee6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the code for Option 1 above. \n",
    "housing_option1 = housing.copy() #This makes a copy of the data to variable housing_option1, so that we don't mess up the original data.\n",
    "\n",
    "housing_option1.dropna(subset=[\"total_bedrooms\"], inplace=True)  # option 1 - dropping the rows where total_bedroom is missing values.\n",
    "\n",
    "housing_option1.info() #look for missing values after rows have been dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e11d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_option2 = housing.copy() #This makes a copy of the data to variable housing_option1, so that we don't mess up the original data.\n",
    "\n",
    "housing_option2.drop(\"total_bedrooms\", axis=1, inplace=True)  # option 2 - dropping the column associated with total_bedrooms\n",
    "\n",
    "housing_option2.info() # checking for missing values in the new data after column has been dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dba58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_option3 = housing.copy() #This makes a copy of the data to variable housing_option1, so that we don't mess up the original data.\n",
    "\n",
    "median = housing[\"total_bedrooms\"].median() # calculating mean of the value for total_bedrooms to use in filling missing values\n",
    "housing_option3[\"total_bedrooms\"].fillna(median, inplace=True)  # option 3 - filling missing values with the median\n",
    "\n",
    "housing_option3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf9695a",
   "metadata": {},
   "source": [
    "#### You can also use `SimpleImputer` from the `sklearn.impute` library to fill missing values with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b28f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\") # initialises the imputer\n",
    "\n",
    "housing_num = housing.select_dtypes(include=[np.number]) ## includes only numeric features in the data\n",
    "\n",
    "imputer.fit(housing_num) #calculates the median for each numeric feature so that the imputer can use them\n",
    "\n",
    "housing_num[:] = imputer.transform(housing_num) # the imputer uses the median to fill the missing values and saves the result in variable X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c1a0b1",
   "metadata": {},
   "source": [
    "### Step 6: Scaling Your Features\n",
    "\n",
    "Machine learning algorithms learn better when similar scales are used across all the features. For example, the numeric range of values for `total_rooms` will be totally different from `median_income`.\n",
    "\n",
    "Test this with he **min** and **max** values after running the pandas `describe()` function. Code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead8cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb924d5b",
   "metadata": {},
   "source": [
    "You can see that all the features have very different ranges. Bringing these in alignment is called **feature scaling**. There are a number of ways to scale features. Scikit-Learn provides something called MinMaxScaler which scales the values to fit into a range defined by you. Below, the code is provided for when you are fitting it into the range from -1 to 1. AI algorithms often like the mean to be placed at zero, so best to set a range with zero as the mid point value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396bcce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler # get the MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1)) # setup an instance of a scaler\n",
    "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)# use the scaler to transform the data housing_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc23108",
   "metadata": {},
   "source": [
    "Alternatively, Scikit-Learn also provides a method called StandardScaler. This method tries normalise the distributional characteristics by considering mean and standard deviation for each feature, and normalising the values to have standard deviation 1. But, even without knowing the mathematical details, we can simply employ the tools provided by `sklearn` - example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "housing_num_std_scaled = std_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bd704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num[:]=std_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018f320",
   "metadata": {},
   "source": [
    "### Step 7: Train a Linear Regression Model\n",
    "\n",
    "In the first instance we will use the data resulting from the `SimpleImputer` (with the **median** as a strategy) and use   `StandardScaler` for scaling the features. Before we train the Linear Regression model for predicting median housing prices for districts, we need to also apply the scaling to the target labels (in our case, the known median housing prices). The code is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483b52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler #This line is not necessary if you ran this prior to running this cell. \n",
    "#We are however including it here for completeness sake.\n",
    "\n",
    "target_scaler = StandardScaler() #instance of Scaler\n",
    "scaled_labels = target_scaler.fit_transform(housing_labels.to_frame()) #calculate the mean and standard deviation and use it to transform the target labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed4b20f",
   "metadata": {},
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc4e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression #get the library from sklearn.linear model\n",
    "\n",
    "model = LinearRegression() #get an instance of the untrained model\n",
    "model.fit(housing_num, scaled_labels)\n",
    "#model.fit(housing[[\"median_income\"]], scaled_labels) #fit it to your data\n",
    "#some_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data\n",
    "\n",
    "#scaled_predictions = model.predict(some_new_data)\n",
    "#predictions = target_scaler.inverse_transform(scaled_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a1e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_new_data = housing_num.iloc[:5] #pretend this is new data\n",
    "#some_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data\n",
    "\n",
    "scaled_predictions = model.predict(some_new_data)\n",
    "predictions = target_scaler.inverse_transform(scaled_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd4851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions, housing_labels.iloc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3980d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – computes the error ratios discussed in the book\n",
    "error_ratios = housing_predictions[:5].round(-2) / housing_labels.iloc[:5].values - 1\n",
    "print(\", \".join([f\"{100 * ratio:.1f}%\" for ratio in error_ratios]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ebd5a",
   "metadata": {},
   "source": [
    "### Step 8: Cross Validation\n",
    "\n",
    "As mentioned in Lecture 4 - pre-recorded lecture - having one training set and one test set to check performance is limited in producing a robust AI model. What you really want to see is a stable performance across many training sets and test sets. IN the first stance you want to test the model on the training set.\n",
    "\n",
    "One way to evaluate your model before testing on the new data (the data you set aside as your test data) is cross validation. This where you split your training data into many pieces, then leave on of the pieces out for testing.The code below does that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf15d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rmses = -cross_val_score(model, housing_num, scaled_labels,\n",
    "                              scoring=\"neg_root_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ea58a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(rmses).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c936dc9a",
   "metadata": {},
   "source": [
    "## Task 4-2: Hand Written Digit Classification\n",
    "\n",
    "As mentioned earlier, as an example, for the hand written digits, we will use a specific kind of neural network model called a **Convolutional Neural Network (CNN)** model. In the lectures, we learned about general neural networks but not CNN. If you want to get a feel for CNNs, you can watch the Stat Quest Video [Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs)](https://www.youtube.com/watch?v=HGwBXDKFk9I). If you feel confident to go deeper, Chapters 19 and 22 of Russell and Norvig's Book [\"Artificial Intelligence: a modern approach\"](https://eleanor.lib.gla.ac.uk/record=b3897063) is an excellent read, not to mention Géron's book [\"Hands-On Machine Learning with Scikit-Learn, Keras & Tensorflow\"](https://eleanor.lib.gla.ac.uk/record=b4094676).\n",
    "\n",
    "For this task, **we will move away from `sklearn` and use `tensorflow` and `keras` instead**. Tensorflow and Keras are popular libraries recognised for their usefulness in building neural networks quickly. Although we already loaded the data from `sklearn`, in the code below, we will get it again from `tensorflow.keras.datasets`. This will allow you to experience getting data from another library and also make it easier to work with the subsequent code because everything will happen with tensorflow. \n",
    "\n",
    "The data is already fairly organised, so, the data cleaning part of the operation can be abbreviated. This is however not a characteristic of image data. It is a characteristic of **curated data** which is not the same as real world messy data (such as the housing data from earlier). \n",
    "\n",
    "The code for importing the libraries and getting the data has been included below. To get these to work, **you will need to have your environment installed with `tensorflow` and `keras`**. In the first line of the first code cell below, you will notice that `tensorflow` is imported as `tf`. This is a recognised convention in the machine learning community. Adopting this convention makes your code more readable for this community. Once you have imported the library that way, `tf` will be used subsequently instead of `tensorflow`.\n",
    "\n",
    "### Step 1: Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89621d",
   "metadata": {},
   "source": [
    "### Step 2: Review What the Data Looks Like  \n",
    "\n",
    "You can review information about what this dataset looks like at the Keras page for the [MNIST digits classification dataset](https://keras.io/api/datasets/mnist/). The page makes it clear that `mnist` above is organised as a data type called **tuple** - something that looks like `(a,b)`. The `a` and `b` are tuples themselves, representing training and test data, respectively. Check first that mnist is a **tuple** with following line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cdcbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(mnist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adf9270",
   "metadata": {},
   "source": [
    "The **Keras webpages are useful** for looking up and getting information about wide range of keras commands you might encouter in machine learning programs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1805f5f4",
   "metadata": {},
   "source": [
    "### Step 3: How to Get the Data\n",
    "\n",
    "To get the data out of `a` and `b`, run the following code. Read the comment for explanation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c621149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = mnist \n",
    "# (X_train_full, y_train_full) is the 'tuple' related to `a` and (X_test, y_test) is the 'tuple' related to `b`.\n",
    "# X_train_full is the full training data and y_train_full are the corresponding labels \n",
    "# - labels indicate what digit the image is of, for example 5 if it is an image of a handwritten 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a62b91a",
   "metadata": {},
   "source": [
    "### Step 4: Scaling the Pixel Values (the features)\n",
    "\n",
    "In dealing with images, there are four main comsiderations that most frequently arise: \n",
    "- 1) input size of the image (height and width in terms of pixels)\n",
    "- 2) whether you want to move the pixels so that the image is centered in the middle\n",
    "- 3) scaling the value of the pixels to be in a specified range. \n",
    "\n",
    "The neural network we will use will works best with pixel values between 0 and 1. Pixels in a black and white image usually have values between 0 and 255. The code below simply rescales these, dividing by 255. There are other ways of scaling this, similar to when we scaled the feature values of the `housing` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c876f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = X_train_full / 255.\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf244fc8",
   "metadata": {},
   "source": [
    "### Step 5: Split the Training Data into Training and Validation Data\n",
    "\n",
    "We already have data split into training and test data. The **validation data is split from the training data** and used to evaluate the performance during training. This is **different from test data** which is completely new data not seen during training or fine tuning. \n",
    "\n",
    "Test data is used for the final test before publishing the results. In fact in many competitions, the test data is **withheld behind an application interface** so that contestants cannot engage in **data snooping**. \n",
    "\n",
    "The code below takes the last 5000 images for validation data. The second line does the same for the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d405ef72",
   "metadata": {},
   "source": [
    "### Step 5: Increasing Dimension to Include Colour Channels\n",
    "\n",
    "An image is usually represented as a (width x height) block of pixels. When presenting your images to the neural network, you need to add an extra dimension to your image representation, to indicate the number of colour channels your images are using. Normally, for a greyscale image this would be 1, while for a RBG colour image it would be 3. \n",
    "\n",
    "All in all you will be submitting something that has shape like `(N, W, H, C)` where `N` is number of images, `W` is the width of any one image, `H` is the height of any one image, and `C` is the number channels (1 for greyscale, 3 for colour). \n",
    "\n",
    "All your images are expected to be the same size as it enters the neural network. \n",
    "\n",
    "The mnist dataset currently has a shape like `(N, W, H)`. Your numpy library allows you to add the required extra dimension. The code below does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d4722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # you won't need to run this line if you ran it before in this notebook. But for completeness.\n",
    "\n",
    "X_train = X_train[..., np.newaxis] #adds a dimension to the image training set - the three dots means keeping everything else the same.\n",
    "X_valid = X_valid[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f3aba",
   "metadata": {},
   "source": [
    "### Step 6: Build the Neural Network and Fit it to the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a69e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Unlike scikit-learn, with tensorflow and keras, the model is built by defining each layer of the neural network.\n",
    "# Below, everytime tf.keras.layers is called it is building in another layer\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d356c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() # not necessary for the machine learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022105d9",
   "metadata": {},
   "source": [
    "The summary above is not easy to read initially but it is a presentation of each layer. The numbers at the bottom tell you how many parameters need learning in this model. The visualisation can be useful later when you get more used to neural networks if you should continue on to Semester 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e414d05",
   "metadata": {},
   "source": [
    "### Step 7: Train and Evaluate the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a2f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc4b587",
   "metadata": {},
   "source": [
    "### Comparing with Another Model\n",
    "\n",
    "Below you are provided with code for using something called **Stochastic Gradient Decent Classifier**. This model applies the stochastic gradient descent optimiser (cf. the **nadam** optimiser used with the CNN above) with any number of algorithms but by default it applies it to a **Support Vector Machine**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ed072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the data again from Scikit-Learn, so that we know the image dimens fit for the model!\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "\n",
    "mnist = fetch_openml('mnist_784', as_frame=False, parser='auto')\n",
    "\n",
    "# getting the data and the categories for the data\n",
    "images = mnist.data\n",
    "categories = mnist.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185aac1",
   "metadata": {},
   "source": [
    "**Normally, we would set aside the test data**. \n",
    "\n",
    "But in this experiement we will abbreviate and use the entire data and evaluate using cross validation, especially since we are not intending, on this occasion, to develop our model with the validation step. **Note that running this might take a while - so be patient!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a621d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "\n",
    "#cross validation on training data for fit accuracy\n",
    "\n",
    "accuracy = cross_val_score(sgd_clf, images, categories, cv=10)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acedfa2",
   "metadata": {},
   "source": [
    "You can see that the accuracies across all the validation runs are far below that of the CNN test results above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e27a131",
   "metadata": {},
   "source": [
    "# Task 5: Reflection\n",
    "\n",
    "That's it! You've reviewed the machine learning workflow. Before you go, let's reflect on a few things together to fill in the gaps!\n",
    "\n",
    "\n",
    "## Task 5-1: Reflecting on the Machine Learning Workflow\n",
    "\n",
    "\n",
    "Get together with your peer group. For the following tasks, you are expected to write a markdown cell describing the workflow required. You are free to include code, but **no Python code is required**. Discuss the following:\n",
    "\n",
    "1. What would you need to do for your code if:\n",
    "\n",
    "- Your were to use your own data (for example, discuss survey data data and photos)?\n",
    "- You were changing\n",
    "    - Your model?\n",
    "    - Your scaling method?\n",
    "    - Your approach to handling missing data?\n",
    "2. What is the significance of cross validation?\n",
    "\n",
    "### Further exploration\n",
    "\n",
    "In this exercise we only considered numerical data from the housing data - that is we left out the feature `ocean_proximity` which is not numerical. Find out about **One Hot Encoding** from Chapter 2 of the [Hands On Machine Learning book](https://eleanor.lib.gla.ac.uk/record=b4094676). Also watch the video on [Word Embedding and Word2Vec](https://www.youtube.com/watch?v=viZrOnJclY0), to get an intuition for **how textual content is transformed into numerical data**.\n",
    "\n",
    "## Task 5-2: Introducing the Tensorflow Playground\n",
    "\n",
    "Before you go, let's play a little bit more with Neural Networks. There is an excellent online resource for this. Go to [playground.tensorflow.org](https://playground.tensorflow.org/). This site allows you to play around with different neural network architecture to see how well they perform in distinguishing data in different formation where data points of the same colour belong to the same class. \n",
    "\n",
    "Change your data type to \"spiral\" by clicking on the picture for spiral data on the lefthand side. \n",
    "- The idea is that the point of orange colour is one class and the ones of blue colour is another class. \n",
    "- As the neural network learns you will see the image on the righthand side change background colour (blue/organge) - the class the neural network thinks the points in those regions belong to.  \n",
    "\n",
    "#### Task 5-2-1: Finding small networks that perform well.\n",
    "\n",
    "- Play around with the interface to get a feel for where everything is. For example, add more hidden layers (each layer is represented as nodes laid out vertically) and/or add nodes in any layer. Do this together in your group. \n",
    "- Try to come up with the smallest network that will bring the training loss down to 0.2 or less. The traning loss is indicated on the right hand side - right underneath the label **Output**.\n",
    "- In a Markdown cell below, describe how many layers with how many nodes you had in your network and how many epochs (indicated on the top lefthand corner) for your best model.\n",
    "\n",
    "#### Task 5-2-2: Examine the patterns displayed in the network nodes (see the image above). \n",
    "\n",
    "Discuss in your group and note down in a markdown cell: \n",
    "- what kinds of patterns the neural network might be learning at different layers and nodes. It is difficult to determine this for certain but you can get some intuition by hovering over the nodes in the tensorflow playground.\n",
    "\n",
    "Markdown cells have been included below for addressing the discussions in Task 5. This is for your convenience - modify as you see fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d01f6a",
   "metadata": {},
   "source": [
    "**Markdown cell for Task 5**\n",
    "\n",
    "1. \n",
    "2.\n",
    "3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a449b9",
   "metadata": {},
   "source": [
    "## Task 5-3 (Optional): Pre-trained Models\n",
    "\n",
    "Before we conclude this notebook, we will momentarily explore the **pre-trained model** VGG-19. This model was trained for computer vision and image classification. It was developed at Oxford but it is often considered to be the next generation model after AlexNet, which won the ImageNet challenge in 2012.\n",
    "\n",
    "The model is introduced here to illustrate an example of a large convolutional neural network, much bigger than that used for MNIST classification task. Note how many more laters are involved, and the total parameters indicated at the bottom is huge. We can talk about this further if you should continue onto the course in Semester 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "\n",
    "model = VGG19() ### this will take some time!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b2700",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c37300",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook you learned about the machine learning pipeline. You reviewed the general workflow from class, and reflected on the workflow in the context of two example cases and data (housing data and minist data). You tried out **Linear Regression** and **Convolutional Neural Net Work**. You also briefly looked at something called a **Support Vector Machine** with **Stochastic Gradient Descent** (not covered in the lectures), comparing the performance for handwritten digit recognition. \n",
    "\n",
    "Any one of these algorithms when looked at in detail, can be quite complex in terms of steps, as seen in the lectures and these labs. However, when using convenient libraries such as `sklearn`, many of them can be implemented in just a few lines. Having said that, where much of the complexity comes in is in preparing the data. And the **data needs more preparing when it is just collected from real world scenarios or sources**.\n",
    "\n",
    "**When data is curated** (such as the MNIST data), there is less to clean and prepare. However, if we are to discuss AI and bias, we need to to critically look at decisions made at the data curation stage. Often these decisions are not as transparent as it could be, which compromises our ability to assess the suitability of datasets, algorithms, and interpretation of results.\n",
    "\n",
    "We also played with the Tensorflow Playground to enhance our intuition for neural networks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
